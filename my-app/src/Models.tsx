import xception from './xception.png'
export default function Models() {
    return <>
        <h1>Introduction to Deepfake Models</h1>
        <h3>First Order Motion Model</h3>
        <p>
            It is the current state-of-the-art model for generating Deepfakes. This model can generate both replacement and reenactment Deepfakes using a source image and a driving video as input. It uses a keypoint detector to extract moving key points from a driving video. Then, it creates a dense motion network to calculate the optical flow of each frame compared to the frame from which the key point was extracted. At last, using the dense motion network and a trained generation module feeding the source video, the deep fake can be generated. This method does not rely on pre-labeled key points or data. Instead, it just deduces the key points using its own detector, allowing a wide range of applications and faster generation time.
        </p>
        <h3>X2-Face Model</h3>
        <p>
            X2-Face is another Deepfake generation model, which requires a source, either an image or a video, and a driving video as input. It uses an embedding network to first transform the source video into a map of vectors, which can then be plotted into the source video to form an embedded face. After changing the driving video into another map of vectors, we can apply the embedded face to it and thus generate a realistic-looking Deepfake video.
            When we compare X2-Face with the First Order Motion Model, the First Order Motion Model makes generation much easier and requires significantly less input than the model mentioned above. It also seems to produce better output when compared with the X2-Face model.
        </p>
        <h3>Capsule Network Model</h3>
        <p>
        Capsule network aims to isolate different facial features, e.g. eye, lips, etc., and build a custom detector for each of the facial features for Deepfake detection. To achieve such a mechanism, every input image for detection is first passed into a feature extractor specified in the VGG-19 architecture. In the literature, only the first 3 layers of the VGG-19 network are used to prevent the bias from the object detection task. The output of the VGG network, representing the learned features of different facial components, is then used to feed into a pre-specified number of ‘capsules’, i.e. dedicated prediction sub-networks for detection purposes. In the default implementation, the number of capsules to be created is 10. Each of the prediction sub-networks consists of 2 back-to-back (2D convolutional layer - batch norm 2D layer - ReLU layer) blocks followed by Statistics Pooling layer consisting of mean and standard deviation operations, and finally 2 sets of (1D convolutional layer - batch norm 1D layer). The above obtained features will be finally passed into 2 (real / fake) separate capsules for the final binary classification, followed by a softmax and a mean layer. Below shows a graphical representation of the above stated architecture. 
        </p>
        <h3>Xeception Network</h3>
        <p>
        Xception network makes use of a combination of convolutional layer, max-pooling layers, and convolution-ReLU blocks for deep-learning purposes. Due to the complexity of the network, the model is not easily interpretable. However, the detailed structure is still displayed below for reference.
        </p>
        <img src={xception}/>
        <h3>Ensemble Network</h3>
        <p>
        There are in total 5 different models trained in this particular approach. While the first one being the Xeception Network, the remaining 4 models are all based on EfficientNet, which are designed for automatic scaling of CNNs. The EfficientNet architecture also has an additional benefit for being lighter-weight and trains faster than Xception. In addition to the EfficientNet architecture, proposes the inclusion of an attention mechanism to learn which part of the input is more relevant for the prediction. Besides, also proposes training on the ‘triplet loss’ of the representation learned after passing through the EfficientNet network. The triplet loss training component effectively aims to maximise the difference between the representation output from EfficientNet with images from different classes, while at the same time minimising the representation output from EfficientNet regarding images of the same class. The formula of triplet loss is denoted by LT=max(0, ++--). Take input of a real image (IR) as an example,+ represents the L2 norm of the representation learned from IR and the representation learned from another sample of the same class (i.e. another real image). - represents the L2 norm of the representation learned from IR and the representation learned from another sample of a different class (i.e. a fake image). Model training with triplet loss is also called Siamese learning, where the final output model is first trained on minimising the triplet loss, and an additional simple classification layer is trained to determine the final prediction (real / fake). In total, it generate the 4 model architecture, with a vanilla EfficientNet (EfficientNetB4), EfficientNet with attention mechanism (EfficientNetB4AutoAtt), EfficientNet with triplet loss (EfficientNetB4-triplet), and EfficientNet with triplet loss and attention mechanism (EfficientNetB4AutoAtt-triplet).
        </p>The model used with the frequency domain features is the simplest among the models implemented in this project. In essence, the model used to determine the final output is either a logistic regression fitter or an SVM. Before passing through the classifiers, the images first undergo the Discrete Fourier Transform (DFT) to sample frequency domain signals from the frame images. As the feature after performing the DFT operation is still in 2D, Azimuthal average operations are applied to the extracted frequency signals to obtain a robust 1D representation of the DFT spectrum. The output 1D features will be used to fit the classifier for prediction. 
        <h3>Frequency Domain Model</h3>
        <p>
        The model used with the frequency domain features is the simplest among the models implemented in this project. In essence, the model used to determine the final output is either a logistic regression fitter or an SVM. Before passing through the classifiers, the images first undergo the Discrete Fourier Transform (DFT) to sample frequency domain signals from the frame images. As the feature after performing the DFT operation is still in 2D, Azimuthal average operations are applied to the extracted frequency signals to obtain a robust 1D representation of the DFT spectrum. The output 1D features will be used to fit the classifier for prediction. 
        </p>

    </>
}
